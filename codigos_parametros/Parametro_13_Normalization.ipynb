{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxQrdM4MhWxG"
      },
      "source": [
        "# PAR√ÅMETRO 13: TIPO DE NORMALIZACI√ìN\n",
        "\n",
        "Este notebook procesa **SOLO el Par√°metro 13** de forma independiente.\n",
        "\n",
        "**Configuraci√≥n base (de Gonzalo):**\n",
        "- IMG_SIZE = 256\n",
        "- BATCH_SIZE = 4\n",
        "- EPOCHS = 10\n",
        "- FILTERS_BASE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJX8ypWWpYbP"
      },
      "outputs": [],
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "  console.log(\"Manteniendo conexi√≥n activa...\");\n",
        "  document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect, 60000)  // Cada 60 segundos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAoba6yigR8T"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS Y SETUP INICIAL\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Conv2DTranspose, Activation, concatenate, Input, LayerNormalization\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8wcTt-XhjAv"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# M√âTRICAS\n",
        "# ============================================================================\n",
        "\n",
        "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (\n",
        "        tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth\n",
        "    )\n",
        "\n",
        "def f1_score(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "    precision = tp / (tp + fp + smooth)\n",
        "    recall = tp / (tp + fn + smooth)\n",
        "    return 2 * precision * recall / (precision + recall + smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_Ttdw9ohmWS"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXPERIMENT MANAGER\n",
        "# ============================================================================\n",
        "\n",
        "class ExperimentManager:\n",
        "    \"\"\"Gestiona experimentos autom√°ticamente\"\"\"\n",
        "\n",
        "    def __init__(self, experiment_name, base_dir=\"/content/drive/MyDrive/Actividad2_Resultados_Damian\"):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.base_dir = base_dir\n",
        "        self.experiment_dir = os.path.join(base_dir, experiment_name)\n",
        "        os.makedirs(self.experiment_dir, exist_ok=True)\n",
        "\n",
        "        self.models_dir = os.path.join(self.experiment_dir, \"models\")\n",
        "        self.logs_dir = os.path.join(self.experiment_dir, \"logs\")\n",
        "        self.plots_dir = os.path.join(self.experiment_dir, \"plots\")\n",
        "        self.metrics_dir = os.path.join(self.experiment_dir, \"metrics\")\n",
        "\n",
        "        for d in [self.models_dir, self.logs_dir, self.plots_dir, self.metrics_dir]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "        print(f\"‚úÖ Experimento '{experiment_name}' inicializado\")\n",
        "        print(f\"üìÅ Resultados en: {self.experiment_dir}\")\n",
        "\n",
        "    def run_experiment(self, model, train_dataset, test_dataset,\n",
        "                       epochs, param_name, param_value):\n",
        "        experiment_id = f\"{param_name}_{param_value}\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"üöÄ EJECUTANDO: {param_name} = {param_value}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        model_path = os.path.join(self.models_dir, f\"best_model_{experiment_id}.keras\")\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1\n",
        "        )\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss', patience=20, mode='min', verbose=1, restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        start_time = datetime.now()\n",
        "        history = model.fit(\n",
        "            train_dataset, epochs=epochs, validation_data=test_dataset,\n",
        "            callbacks=[checkpoint, early_stopping], verbose=1\n",
        "        )\n",
        "        end_time = datetime.now()\n",
        "        training_time = (end_time - start_time).total_seconds() / 60\n",
        "\n",
        "        metrics = self._extract_metrics(history, param_name, param_value, training_time)\n",
        "        self._save_all(history, metrics, experiment_id, param_name, param_value)\n",
        "\n",
        "        print(f\"\\n‚úÖ Completado en {training_time:.2f} min\")\n",
        "        return history, metrics\n",
        "\n",
        "    def _extract_metrics(self, history, param_name, param_value, training_time):\n",
        "        return {\n",
        "            'experiment_name': self.experiment_name,\n",
        "            'parameter_name': param_name,\n",
        "            'parameter_value': param_value,\n",
        "            'training_time_minutes': round(training_time, 2),\n",
        "            'best_val_iou': float(max(history.history.get('val_iou_metric', [0]))),\n",
        "            'best_val_dice': float(max(history.history.get('val_dice_coef', [0]))),\n",
        "            'best_val_f1': float(max(history.history.get('val_f1_score', [0]))),\n",
        "            'best_val_accuracy': float(max(history.history.get('val_accuracy', [0]))),\n",
        "            'best_val_loss': float(min(history.history.get('val_loss', [999]))),\n",
        "            'best_train_accuracy': float(max(history.history.get('accuracy', [0]))),\n",
        "            'best_train_loss': float(min(history.history.get('loss', [999]))),\n",
        "            'total_epochs': len(history.history['loss'])\n",
        "        }\n",
        "\n",
        "    def _save_all(self, history, metrics, experiment_id, param_name, param_value):\n",
        "        with open(os.path.join(self.metrics_dir, f\"metrics_{experiment_id}.json\"), 'w') as f:\n",
        "            json.dump(metrics, f, indent=4)\n",
        "\n",
        "        with open(os.path.join(self.logs_dir, f\"history_{experiment_id}.pkl\"), 'wb') as f:\n",
        "            pickle.dump(history.history, f)\n",
        "\n",
        "        with open(os.path.join(self.logs_dir, f\"epoch_logs_{experiment_id}.txt\"), 'w') as f:\n",
        "            f.write(f\"LOGS DE ENTRENAMIENTO - {param_name} = {param_value}\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\\n\")\n",
        "            for epoch in range(len(history.history['loss'])):\n",
        "                f.write(f\"Epoch {epoch+1}\\n\")\n",
        "                f.write(f\"  Loss: {history.history['loss'][epoch]:.6f}\\n\")\n",
        "                f.write(f\"  Val Loss: {history.history['val_loss'][epoch]:.6f}\\n\")\n",
        "                f.write(f\"  Accuracy: {history.history['accuracy'][epoch]:.6f}\\n\")\n",
        "                f.write(f\"  Val Accuracy: {history.history['val_accuracy'][epoch]:.6f}\\n\\n\")\n",
        "\n",
        "    def compare_experiments(self, histories_dict, param_name):\n",
        "        metrics = ['loss', 'accuracy', 'iou_metric', 'dice_coef', 'f1_score']\n",
        "        fig, axes = plt.subplots(len(metrics), 2, figsize=(15, 5*len(metrics)))\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            for param_value, history in histories_dict.items():\n",
        "                if metric in history.history:\n",
        "                    axes[i, 0].plot(history.history[metric],\n",
        "                                   label=f'{param_name}={param_value}', marker='o', markersize=4)\n",
        "            axes[i, 0].set_title(f'Training {metric.upper()}', fontweight='bold')\n",
        "            axes[i, 0].set_xlabel('Epoch')\n",
        "            axes[i, 0].set_ylabel(metric.replace('_', ' ').title())\n",
        "            axes[i, 0].legend()\n",
        "            axes[i, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            val_metric = f'val_{metric}'\n",
        "            for param_value, history in histories_dict.items():\n",
        "                if val_metric in history.history:\n",
        "                    axes[i, 1].plot(history.history[val_metric],\n",
        "                                   label=f'{param_name}={param_value}', marker='o', markersize=4)\n",
        "            axes[i, 1].set_title(f'Validation {metric.upper()}', fontweight='bold')\n",
        "            axes[i, 1].set_xlabel('Epoch')\n",
        "            axes[i, 1].set_ylabel(metric.replace('_', ' ').title())\n",
        "            axes[i, 1].legend()\n",
        "            axes[i, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_path = os.path.join(self.plots_dir, f\"comparison_{param_name}.png\")\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\nüìà Gr√°fica guardada en: {plot_path}\")\n",
        "        plt.show()\n",
        "\n",
        "    def generate_summary_table(self, metrics_list):\n",
        "        df = pd.DataFrame(metrics_list)\n",
        "        df = df[['parameter_name', 'parameter_value', 'best_val_iou', 'best_val_dice',\n",
        "                 'best_val_f1', 'best_val_accuracy', 'best_val_loss', 'training_time_minutes']]\n",
        "        df.columns = ['Par√°metro', 'Valor', 'Val IoU', 'Val Dice', 'Val F1',\n",
        "                      'Val Accuracy', 'Val Loss', 'Tiempo (min)']\n",
        "\n",
        "        df.to_csv(os.path.join(self.metrics_dir, \"summary_table.csv\"), index=False)\n",
        "        with open(os.path.join(self.metrics_dir, \"summary_table.txt\"), 'w') as f:\n",
        "            f.write(df.to_string(index=False))\n",
        "\n",
        "        print(\"\\nüìã TABLA RESUMEN:\")\n",
        "        print(df.to_string(index=False))\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywXi2t90hm_P"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FUNCIONES DE CARGA\n",
        "# ============================================================================\n",
        "\n",
        "def load_data(path, img_size):\n",
        "    images = []\n",
        "    masks = []\n",
        "    num_images = 100\n",
        "\n",
        "    for img_files, mask_files in zip(\n",
        "        os.listdir(path + \"/Original\")[:num_images],\n",
        "        os.listdir(path + \"/Ground truth\")[:num_images]\n",
        "    ):\n",
        "        img_path = os.path.join(path + \"/Original\", img_files)\n",
        "        mask_path = os.path.join(path + \"/Ground truth\", mask_files)\n",
        "\n",
        "        img = load_img(img_path, target_size=img_size)\n",
        "        mask = load_img(mask_path, target_size=img_size, color_mode=\"grayscale\")\n",
        "\n",
        "        img = img_to_array(img) / 255.0\n",
        "        mask = img_to_array(mask) / 255.0\n",
        "\n",
        "        images.append(img)\n",
        "        masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "def tf_dataset(images, masks, batch_size=4, augment_fn=None):\n",
        "    def generator():\n",
        "        for img, mask in zip(images, masks):\n",
        "            if augment_fn:\n",
        "                img, mask = augment_fn(img, mask)\n",
        "            yield img, mask\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(256, 256, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "    return dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o2CunzRhqmi"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ARQUITECTURAS UNET CON DIFERENTES NORMALIZACIONES\n",
        "# ============================================================================\n",
        "\n",
        "def conv_block_batch(input, filters, kernel_size=3):\n",
        "    \"\"\"Con BatchNormalization\"\"\"\n",
        "    x = Conv2D(filters, kernel_size, padding=\"same\")(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def conv_block_none(input, filters, kernel_size=3):\n",
        "    \"\"\"Sin normalizaci√≥n\"\"\"\n",
        "    x = Conv2D(filters, kernel_size, padding=\"same\")(input)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def conv_block_layer(input, filters, kernel_size=3):\n",
        "    \"\"\"Con LayerNormalization\"\"\"\n",
        "    x = Conv2D(filters, kernel_size, padding=\"same\")(input)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv2D(filters, kernel_size, padding=\"same\")(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def encoder_block_custom(input, filters, conv_block_fn, kernel_size=3):\n",
        "    x = conv_block_fn(input, filters, kernel_size)\n",
        "    p = MaxPooling2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block_custom(input, skip, filters, conv_block_fn, kernel_size=3):\n",
        "    x = Conv2DTranspose(filters, 2, strides=2, padding=\"same\")(input)\n",
        "    x = concatenate([x, skip])\n",
        "    x = conv_block_fn(x, filters, kernel_size)\n",
        "    return x\n",
        "\n",
        "def build_model_4_layers(img_size=256, filters_base=64, conv_block_fn=conv_block_batch, kernel_size=3):\n",
        "    input_layer = Input(shape=(img_size, img_size, 3))\n",
        "\n",
        "    x1, p1 = encoder_block_custom(input_layer, filters_base, conv_block_fn, kernel_size)\n",
        "    x2, p2 = encoder_block_custom(p1, filters_base * 2, conv_block_fn, kernel_size)\n",
        "    x3, p3 = encoder_block_custom(p2, filters_base * 4, conv_block_fn, kernel_size)\n",
        "    x4, p4 = encoder_block_custom(p3, filters_base * 8, conv_block_fn, kernel_size)\n",
        "\n",
        "    a1 = conv_block_fn(p4, filters_base * 16, kernel_size)\n",
        "\n",
        "    d1 = decoder_block_custom(a1, x4, filters_base * 8, conv_block_fn, kernel_size)\n",
        "    d2 = decoder_block_custom(d1, x3, filters_base * 4, conv_block_fn, kernel_size)\n",
        "    d3 = decoder_block_custom(d2, x2, filters_base * 2, conv_block_fn, kernel_size)\n",
        "    d4 = decoder_block_custom(d3, x1, filters_base, conv_block_fn, kernel_size)\n",
        "\n",
        "    output = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "    return Model(input_layer, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNAnuflEhs2U"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CARGAR DATOS (UNA SOLA VEZ)\n",
        "# ============================================================================\n",
        "\n",
        "import kagglehub\n",
        "download_path = kagglehub.dataset_download(\"nikitamanaenkov/fundus-image-dataset-for-vessel-segmentation\")\n",
        "print(\"Path to dataset files:\", download_path)\n",
        "\n",
        "# Cargar datos\n",
        "IMG_SIZE = 256\n",
        "print(f\"üì• Cargando datos con IMG_SIZE={IMG_SIZE}...\")\n",
        "x_train, y_train = load_data(f\"{download_path}/train\", img_size=(IMG_SIZE, IMG_SIZE))\n",
        "x_test, y_test = load_data(f\"{download_path}/test\", img_size=(IMG_SIZE, IMG_SIZE))\n",
        "print(f\"‚úÖ Datos cargados: train {x_train.shape}, test {x_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XuAt1jXhvCx"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DEFINIR CONSTANTES\n",
        "# ============================================================================\n",
        "\n",
        "BATCH_SIZE = 4   # Mejor de Gonzalo\n",
        "EPOCHS = 10      # Mejor de Gonzalo\n",
        "FILTERS_BASE = 64\n",
        "\n",
        "print(f\"\\nüìä Configuraci√≥n:\")\n",
        "print(f\"  - IMG_SIZE: {IMG_SIZE}\")\n",
        "print(f\"  - BATCH_SIZE: {BATCH_SIZE}\")\n",
        "print(f\"  - EPOCHS: {EPOCHS}\")\n",
        "print(f\"  - FILTERS_BASE: {FILTERS_BASE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA2wHwz3hzrP"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PAR√ÅMETRO 13: TIPO DE NORMALIZACI√ìN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PAR√ÅMETRO 13: TIPO DE NORMALIZACI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "manager13 = ExperimentManager(\"Parametro_13_Normalization\")\n",
        "\n",
        "normalization_configs = {\n",
        "    'Batch': conv_block_batch,\n",
        "    'None': conv_block_none,\n",
        "    'Layer': conv_block_layer\n",
        "}\n",
        "\n",
        "histories_norm = {}\n",
        "metrics_list_norm = []\n",
        "\n",
        "train_dataset = tf_dataset(x_train, y_train, batch_size=BATCH_SIZE)\n",
        "test_dataset = tf_dataset(x_test, y_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "for norm_name, conv_block_fn in normalization_configs.items():\n",
        "    print(f\"\\nüîÑ Probando normalizaci√≥n: {norm_name}\")\n",
        "\n",
        "    model = build_model_4_layers(img_size=IMG_SIZE, filters_base=FILTERS_BASE, conv_block_fn=conv_block_fn)\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\", iou_metric, dice_coef, f1_score]\n",
        "    )\n",
        "\n",
        "    history, metrics = manager13.run_experiment(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        test_dataset=test_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        param_name=\"normalization\",\n",
        "        param_value=norm_name\n",
        "    )\n",
        "\n",
        "    histories_norm[norm_name] = history\n",
        "    metrics_list_norm.append(metrics)\n",
        "\n",
        "manager13.compare_experiments(histories_norm, param_name=\"normalization\")\n",
        "summary_df_norm = manager13.generate_summary_table(metrics_list_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5L3LG2EyZ8g"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZACI√ìN CUALITATIVA - PAR√ÅMETRO 13: normalization\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\nüñºÔ∏è Generando visualizaci√≥n cualitativa...\")\n",
        "\n",
        "# Seleccionar una imagen de test\n",
        "sample_index = 0\n",
        "test_image = x_test[sample_index]\n",
        "test_mask = y_test[sample_index]\n",
        "\n",
        "# Crear figura\n",
        "num_models = len(histories_norm)\n",
        "fig, axes = plt.subplots(1, num_models + 2, figsize=(4 * (num_models + 2), 4))\n",
        "\n",
        "# Imagen original\n",
        "axes[0].imshow(test_image)\n",
        "axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Ground truth\n",
        "axes[1].imshow(test_mask[:, :, 0], cmap='gray')\n",
        "axes[1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Predicciones de cada modelo\n",
        "for idx, (param_value, history) in enumerate(histories_norm.items()):\n",
        "    # Cargar el mejor modelo guardado\n",
        "    model_path = os.path.join(manager13.models_dir, f\"best_model_normalization_{param_value}.keras\")\n",
        "\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(\n",
        "            model_path,\n",
        "            custom_objects={\n",
        "                'iou_metric': iou_metric,\n",
        "                'dice_coef': dice_coef,\n",
        "                'f1_score': f1_score\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Predecir\n",
        "        test_image_expanded = np.expand_dims(test_image, axis=0)\n",
        "        prediction = model.predict(test_image_expanded, verbose=0)[0]\n",
        "\n",
        "        # Mostrar\n",
        "        axes[idx + 2].imshow(prediction[:, :, 0], cmap='gray', vmin=0, vmax=1)\n",
        "        axes[idx + 2].set_title(f'{param_value}', fontsize=12, fontweight='bold')\n",
        "        axes[idx + 2].axis('off')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error cargando modelo {param_value}: {e}\")\n",
        "        axes[idx + 2].text(0.5, 0.5, 'Error', ha='center', va='center')\n",
        "        axes[idx + 2].set_title(f'{param_value}', fontsize=12, fontweight='bold')\n",
        "        axes[idx + 2].axis('off')\n",
        "\n",
        "plt.suptitle(f'Comparaci√≥n Cualitativa - Normalization (Batch, None, Layer)',\n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Guardar\n",
        "visual_path = os.path.join(manager13.plots_dir, \"visual_comparison_normalization.png\")\n",
        "plt.savefig(visual_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"‚úÖ Visualizaci√≥n guardada en: {visual_path}\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ PAR√ÅMETRO 13 COMPLETADO\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_type": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
